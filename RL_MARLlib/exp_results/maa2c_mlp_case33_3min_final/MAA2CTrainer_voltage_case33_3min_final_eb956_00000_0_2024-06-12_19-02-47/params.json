{
  "batch_mode": "truncate_episodes",
  "entropy_coeff": 0.01,
  "env": "voltage_case33_3min_final",
  "evaluation_interval": 50,
  "framework": "torch",
  "lambda": 1.0,
  "lr": 0.0005,
  "model": {
    "custom_model": "Centralized_Critic_Model",
    "custom_model_config": {
      "agent_level_batch_update": false,
      "agent_name_ls": [
        "agent_0",
        "agent_1",
        "agent_2",
        "agent_3",
        "agent_4",
        "agent_5"
      ],
      "algorithm": "maa2c",
      "checkpoint_end": true,
      "checkpoint_freq": 100,
      "env": "voltage",
      "env_args": {
        "action_bias": 0.0,
        "action_scale": 0.8,
        "data_path": "/home/yos/MARLlib/marllib/patch/dpn/var_voltage_control/data/case33_3min_final",
        "demand_scale": 1.0,
        "dq_dv_weight": null,
        "episode_limit": 240,
        "history": 1,
        "line_weight": null,
        "map_name": "case33_3min_final",
        "mode": "distributed",
        "pv_scale": 1.0,
        "q_weight": 0.1,
        "reset_action": true,
        "seed": 0,
        "state_space": [
          "pv",
          "demand",
          "reactive",
          "vm_pu",
          "va_degree"
        ],
        "v_lower": 0.95,
        "v_upper": 1.05,
        "voltage_barrier_type": "l1",
        "voltage_weight": 1.0
      },
      "episode_limit": 240,
      "evaluation_interval": 50,
      "force_coop": false,
      "framework": "torch",
      "global_state_flag": true,
      "local_dir": "",
      "local_mode": true,
      "mask_flag": false,
      "model_arch_args": {
        "core_arch": "mlp",
        "fc_layer": 2,
        "hidden_state_size": 256,
        "out_dim_fc_0": 128,
        "out_dim_fc_1": 64
      },
      "num_agents": 6,
      "num_cpus_per_worker": 1,
      "num_gpus": 1,
      "num_gpus_per_worker": 0,
      "num_workers": 4,
      "opp_action_in_cc": true,
      "policy_mapping_info": {
        "all_scenario": {
          "all_agents_one_policy": true,
          "description": "voltage control all scenarios",
          "one_agent_one_policy": true,
          "team_prefix": [
            "agent_"
          ]
        }
      },
      "restore_path": {
        "model_path": "",
        "params_path": ""
      },
      "seed": 321,
      "share_policy": "group",
      "space_act": "Box([-0.8], [0.8], (1,), float32)",
      "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100.], (50,), float32), state:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100.], (144,), float32))",
      "stop_iters": 9999999,
      "stop_reward": 999999,
      "stop_timesteps": 2000000
    },
    "max_seq_len": 240
  },
  "multiagent": {
    "policies": "{'shared_policy'}",
    "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7ff30f2d4d30>"
  },
  "num_gpus": 1,
  "num_gpus_per_worker": 0,
  "num_workers": 4,
  "seed": 321,
  "simple_optimizer": false,
  "train_batch_size": 2400,
  "use_gae": true,
  "vf_loss_coeff": 1.0
}